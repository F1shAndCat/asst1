# T5

## 1

The speedup is very trivial,below 1,1x.

As it says above,the saxpy's mission is to only compute merely one time.It means that it's a I/O heavy code rather than a computation heavy code.

That means both the influence using vectorization optimization and the influence using multicore parallelization will pay little help,and since the calculation optimization is already diffused by vectorization,the multicore parallelization will see bare optimization.

No.Now the CPU's processing speed is actually faster than the bus speed.So the total speed depends on the bus speed.Then we can't do a multicore parallelization which is a cpu speedup method,to improve the bus speed bottleneck.

However,we can change the iterating method,but that will make the iterating method between ispc and ispc method difference,stands that optimization doesn't comes from multicore parallelization.

jutification:

changing thread numbers won't change the speedup.

adding computation instructions will explicitly increase the speedup.

## 2

Because in order to make the CPU cache and Memory data  syncronize,when writing over a memory unit,CPU will find in its'  cache,if the memory unit is hit,that's ok,CPU only need to change the data in cache and write it back to memory.But when cache miss appears,CPU had to find the memory unit and put the data into its cache,no matter whether it will use the data or not.

So the bandwidth in total is $4Nbytes$ ,`result`  2times ,X 1time and Y 1time.

## 3

We can reduce the iteration/calculation ratio to speedup the IO rate.

*Thus we can directly use loop unrollng to accelerate.*

*If we're in C++ main we can use #pragma GCC 'unroll-loop' to reduce the iteration/calculation ratio,but we're in ISPC,So we can just chose a reasonable number and unroll it manually. (Ignore this)*

\* Just Found out that ISPC has optimizied the iteration because the bandwith dereased when i used `programcount` coding way.So we can only turn to the cpu cache since we we found that if cache hit the bandwidth will increase significantly.(Actually this helps me solve #2 because when i'm coding #3 I found that cache hit significantly increase the bandwidth)

 After solving #2 ,we've known from #2 that there is actually a toxic memory access,namely the `result` memory reading.So I try to avoid that access,

and it turns out that it runs well, there is about 33% increase in speed,which is in our expectation.

And by the way ISPC code seems to have no api access to directy change the memory unit,So I Used AVX/AVX2 code to imple it.And remember to use `align_alloc` to distribute the memory as it will get a `Segment Fault` verdict when the float array isn't well aligned when using `_mm256_stream_ps`.
